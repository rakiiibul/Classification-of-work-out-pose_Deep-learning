{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detections(cap):\n",
    "    '''# 1. Make Some Detections with a video # '''\n",
    "    if (cap.isOpened() == False):\n",
    "        print(\"Error opening the video file.\")\n",
    "    else:\n",
    "        input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f'Frames per second: {input_fps}')\n",
    "        print(f'Frame count: {frame_count}')\n",
    "    \n",
    "    # Color difine\n",
    "    #color_face1 = (71, 146, 253)\n",
    "    color_face2 = (71, 146, 253)\n",
    "    color_r_hand1 = (71, 146, 253)\n",
    "    color_r_hand2 = (71, 146, 253)\n",
    "    color_l_hand1 = (71, 146, 253)\n",
    "    color_l_hand2 = (71, 146, 253)\n",
    "    color_pose1 = (71, 146, 253)\n",
    "    color_pose2 = (71, 146, 253)\n",
    "\n",
    "\n",
    "    mp_drawing = mp.solutions.drawing_utils # Drawing helpers.\n",
    "    mp_holistic = mp.solutions.holistic     # Mediapipe Solutions.\n",
    "\n",
    "    # Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret == True:\n",
    "                # Recolor Feed\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False        \n",
    "\n",
    "                # Make Detections\n",
    "                results = holistic.process(image)\n",
    "                # print(results.face_landmarks)\n",
    "\n",
    "                # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "                # Recolor image back to BGR for rendering\n",
    "                image.flags.writeable = True   \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                '''# 1. Draw face landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "                    mp_drawing.DrawingSpec(color=color_face1, thickness=1, circle_radius=1),\n",
    "                    mp_drawing.DrawingSpec(color=color_face2, thickness=1, circle_radius=1)\n",
    "                )\n",
    "\n",
    "                # 2. Right hand\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                    mp_drawing.DrawingSpec(color=color_r_hand1, thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=color_r_hand2, thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "                # 3. Left Hand\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                    mp_drawing.DrawingSpec(color=color_l_hand1, thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=color_l_hand2, thickness=2, circle_radius=2)\n",
    "                )'''\n",
    "\n",
    "                # 4. Pose Detections\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                    mp_drawing.DrawingSpec(color=(71, 146, 253), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(71, 146, 253), thickness=2, circle_radius=2)\n",
    "                )\n",
    "            \n",
    "\n",
    "                cv2.imshow('Raw Video Feed', image)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    print('Done.')\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_pose_csv(cap, create_csv):\n",
    "    ''' Create pose detections csv  with a video.'''\n",
    "    if (cap.isOpened() == False):\n",
    "        print(\"\\nError opening the video file.\")\n",
    "        return\n",
    "    else:\n",
    "        pass\n",
    "    # Color difine\n",
    "    color_pose1 = (245,117,66)\n",
    "    color_pose2 = (245,66,230)\n",
    "\n",
    "    mp_drawing = mp.solutions.drawing_utils # Drawing helpers.\n",
    "    mp_holistic = mp.solutions.holistic     # Mediapipe Solutions.\n",
    "\n",
    "    # Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret == True:\n",
    "                # Recolor Feed\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False        \n",
    "\n",
    "                # Make Detections\n",
    "                results = holistic.process(image)\n",
    "\n",
    "                # Recolor image back to BGR for rendering\n",
    "                image.flags.writeable = True   \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Pose Detections\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                    mp_drawing.DrawingSpec(color=color_pose1, thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=color_pose2, thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    num_coords = len(results.pose_landmarks.landmark) # num_coords: 33\n",
    "\n",
    "                    landmarks = ['class'] # Create first rows data.\n",
    "                    for val in range(1, num_coords+1):\n",
    "                        landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]\n",
    "                    \n",
    "                    # E.g., (pose+face)2005=1+501*4, (pose+r_hand)217=1+54*4, 133=1+33*4\n",
    "                    # print(f'len(landmarks): {len(landmarks)}')\n",
    "\n",
    "                    # Define first class rows in csv file.\n",
    "                    with open(create_csv, mode='w', newline='') as f:\n",
    "                        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                        csv_writer.writerow(landmarks)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                cv2.imshow('Raw Video Feed', image)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    print(f'\\nCreate {dataset_csv_file} done! \\n\\nNow you can run again.')\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_record_coordinates(cap, class_name, export_csv):\n",
    "    if (cap.isOpened() == False):\n",
    "        print(\"Error opening the video file.\")\n",
    "    else:\n",
    "        input_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f'Frames per second: {input_fps}')\n",
    "        print(f'Frame count: {frame_count}')\n",
    "\n",
    "    mp_drawing = mp.solutions.drawing_utils # Drawing helpers.\n",
    "    mp_holistic = mp.solutions.holistic     # Mediapipe Solutions.\n",
    "\n",
    "    # Initiate holistic model\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret == True:\n",
    "                # Recolor Feed\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                image.flags.writeable = False        \n",
    "\n",
    "                # Make Detections\n",
    "                results = holistic.process(image)\n",
    "                # print(results.face_landmarks)\n",
    "\n",
    "                # Recolor image back to BGR for rendering\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                #  Pose Detections\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                )\n",
    "                # Export coordinates\n",
    "                try:\n",
    "                    # Extract Pose landmarks\n",
    "                    pose = results.pose_landmarks.landmark\n",
    "                    pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "\n",
    "                    # Extract Face landmarks\n",
    "                    # face = results.face_landmarks.landmark\n",
    "                    row = pose_row\n",
    "\n",
    "                    # Append class name.\n",
    "                    row.insert(0, class_name)\n",
    "\n",
    "                    # Export to CSV\n",
    "                    with open(export_csv, mode='a', newline='') as f:\n",
    "                        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                        csv_writer.writerow(row) \n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    print('Add done!\\n -------------------')\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    # check_csv_contents(file=export_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_csv_contents(file=export_csv)\n",
    "def check_csv_contents(file):\n",
    "    df = pd.read_csv(file)\n",
    "    print(f'Top5 datas: \\n{df.head()}')\n",
    "    print(f'Last5 datas: \\n{df.tail()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.csv: Not exist.\n",
      "\n",
      "Initiate creating a csv file....\n",
      "\n",
      "\n",
      "Error opening the video file.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "n = 0\n",
    "\n",
    "    # 0: cat_camel, 1: bridge_exercise, 2: heel_raise\n",
    "    category = [0, 1, 2]\n",
    "    video_file_name = 'cat_camel' + '2'\n",
    "    \n",
    "    # Add n categories of pose.\n",
    "    add_class = category[n]\n",
    "\n",
    "    # Can create train dataset or test dataset.\n",
    "    dataset_csv_file = 'data.csv'\n",
    "\n",
    "    video_path = \"./\" + video_file_name +\".mp4\"\n",
    "    output_video = video_file_name + \"_out.mp4\"\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if os.path.isfile(dataset_csv_file):\n",
    "        print (f'{dataset_csv_file}: Exist.')\n",
    "        print(f'Add class: {add_class} \\n-----------------')\n",
    "\n",
    "        add_record_coordinates(cap=cap, class_name=add_class, export_csv=dataset_csv_file)\n",
    "    else:\n",
    "        print (f'{dataset_csv_file}: Not exist.')\n",
    "        print('\\nInitiate creating a csv file....\\n')\n",
    "        create_pose_csv(cap, create_csv=dataset_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] =  \"tf.keras\"\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsvDataset:\n",
    "    def __init__(self, file):\n",
    "        self.dataframe = pd.read_csv(file)\n",
    "        self.val_df = None\n",
    "        self.train_df = None\n",
    "        self.val_ds = None\n",
    "        self.train_ds = None\n",
    "\n",
    "    def csv_preprocessing(self):\n",
    "        \n",
    "        df2 = self.dataframe.copy()\n",
    "\n",
    "        columns_removed = [\n",
    "            'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11',\n",
    "            'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11',\n",
    "            'z1', 'z2', 'z3', 'z4', 'z5', 'z6', 'z7', 'z8', 'z9', 'z10', 'z11',\n",
    "            'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11',\n",
    "        ]\n",
    "\n",
    "        df2 = df2.drop(columns_removed, axis = 'columns')\n",
    "        return df2\n",
    "    \n",
    "    def df_to_datasets(self, dataframe, target):\n",
    "        # \n",
    "        self.val_df = dataframe.sample(frac=0.2, random_state=1337)\n",
    "        # drop the colum 1 of 'class'.\n",
    "        self.train_df = dataframe.drop(self.val_df.index)\n",
    "\n",
    "        train_df = self.train_df.copy()\n",
    "        val_df = self.val_df.copy()\n",
    "\n",
    "        train_labels = train_df.pop(target)\n",
    "        val_labels = val_df.pop(target)\n",
    "        \n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((dict(train_df), train_labels))\n",
    "        self.val_ds = tf.data.Dataset.from_tensor_slices((dict(val_df), val_labels))\n",
    "        self.train_ds = self.train_ds.shuffle(buffer_size=len(self.train_ds))\n",
    "        self.val_ds = self.val_ds.shuffle(buffer_size=len(self.val_ds))\n",
    "\n",
    "        return self.train_ds, self.val_ds\n",
    "\n",
    "\n",
    "class EncodeFeatures:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_ds = None\n",
    "            \n",
    "    def numerical_feature(self, feature, name, dataset):\n",
    "        # Create a Normalization layer for our feature\n",
    "        normalizer = Normalization()\n",
    "\n",
    "        # Prepare a Dataset that only yields our feature\n",
    "        self.feature_ds = dataset.map(lambda x, y: x[name])\n",
    "        self.feature_ds = self.feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "        # Learn the statistics of the data\n",
    "        normalizer.adapt(self.feature_ds)\n",
    "\n",
    "        # Normalize the input feature\n",
    "        encoded_feature = normalizer(feature)\n",
    "        return encoded_feature\n",
    "\n",
    "\n",
    "def point():\n",
    "    x = [\n",
    "        'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22',\n",
    "        'x23', 'x24', 'x25', 'x26', 'x27', 'x28', 'x29', 'x30', 'x31', 'x32', 'x33',\n",
    "    ]\n",
    "    y = [\n",
    "        'y12', 'y13', 'y14', 'y15', 'y16', 'y17', 'y18', 'y19', 'y20', 'y21', 'y22',\n",
    "        'y23', 'y24', 'y25', 'y26', 'y27', 'y28', 'y29', 'y30', 'y31', 'y32', 'y33',\n",
    "    ]\n",
    "    z = [\n",
    "        'z12', 'z13', 'z14', 'z15', 'z16', 'z17', 'z18', 'z19', 'z20', 'z21', 'z22',\n",
    "        'z23', 'z24', 'z25', 'z26', 'z27', 'z28', 'z29', 'z30', 'z31', 'z32', 'z33',\n",
    "    ]\n",
    "    v = [\n",
    "        'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22',\n",
    "        'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30', 'v31', 'v32', 'v33',\n",
    "    ]\n",
    "    coords = [x, y, z, v]\n",
    "    return coords\n",
    "\n",
    "\n",
    "def input_features():\n",
    "\n",
    "    coords = point()\n",
    "    # point 12 ~ 33\n",
    "    # Numerical features\n",
    "    x12 = keras.Input(shape=(1,), name=coords[0][0])\n",
    "    x13 = keras.Input(shape=(1,), name=coords[0][1])\n",
    "    x14 = keras.Input(shape=(1,), name=coords[0][2])\n",
    "    x15 = keras.Input(shape=(1,), name=coords[0][3])\n",
    "    x16 = keras.Input(shape=(1,), name=coords[0][4])\n",
    "    x17 = keras.Input(shape=(1,), name=coords[0][5]) \n",
    "    x18 = keras.Input(shape=(1,), name=coords[0][6])\n",
    "    x19 = keras.Input(shape=(1,), name=coords[0][7])\n",
    "    x20 = keras.Input(shape=(1,), name=coords[0][8])\n",
    "    x21 = keras.Input(shape=(1,), name=coords[0][9])\n",
    "    x22 = keras.Input(shape=(1,), name=coords[0][10])\n",
    "    x23 = keras.Input(shape=(1,), name=coords[0][11])\n",
    "    x24 = keras.Input(shape=(1,), name=coords[0][12])\n",
    "    x25 = keras.Input(shape=(1,), name=coords[0][13])\n",
    "    x26 = keras.Input(shape=(1,), name=coords[0][14])\n",
    "    x27 = keras.Input(shape=(1,), name=coords[0][15])\n",
    "    x28 = keras.Input(shape=(1,), name=coords[0][16])\n",
    "    x29 = keras.Input(shape=(1,), name=coords[0][17])\n",
    "    x30 = keras.Input(shape=(1,), name=coords[0][18])\n",
    "    x31 = keras.Input(shape=(1,), name=coords[0][19])\n",
    "    x32 = keras.Input(shape=(1,), name=coords[0][20])\n",
    "    x33 = keras.Input(shape=(1,), name=coords[0][21])\n",
    "\n",
    "    y12 = keras.Input(shape=(1,), name=coords[1][0])\n",
    "    y13 = keras.Input(shape=(1,), name=coords[1][1])\n",
    "    y14 = keras.Input(shape=(1,), name=coords[1][2])\n",
    "    y15 = keras.Input(shape=(1,), name=coords[1][3])\n",
    "    y16 = keras.Input(shape=(1,), name=coords[1][4])\n",
    "    y17 = keras.Input(shape=(1,), name=coords[1][5]) \n",
    "    y18 = keras.Input(shape=(1,), name=coords[1][6])\n",
    "    y19 = keras.Input(shape=(1,), name=coords[1][7])\n",
    "    y20 = keras.Input(shape=(1,), name=coords[1][8])\n",
    "    y21 = keras.Input(shape=(1,), name=coords[1][9])\n",
    "    y22 = keras.Input(shape=(1,), name=coords[1][10])\n",
    "    y23 = keras.Input(shape=(1,), name=coords[1][11])\n",
    "    y24 = keras.Input(shape=(1,), name=coords[1][12])\n",
    "    y25 = keras.Input(shape=(1,), name=coords[1][13])\n",
    "    y26 = keras.Input(shape=(1,), name=coords[1][14])\n",
    "    y27 = keras.Input(shape=(1,), name=coords[1][15])\n",
    "    y28 = keras.Input(shape=(1,), name=coords[1][16])\n",
    "    y29 = keras.Input(shape=(1,), name=coords[1][17])\n",
    "    y30 = keras.Input(shape=(1,), name=coords[1][18])\n",
    "    y31 = keras.Input(shape=(1,), name=coords[1][19])\n",
    "    y32 = keras.Input(shape=(1,), name=coords[1][20])\n",
    "    y33 = keras.Input(shape=(1,), name=coords[1][21])\n",
    "\n",
    "    z12 = keras.Input(shape=(1,), name=coords[2][0])\n",
    "    z13 = keras.Input(shape=(1,), name=coords[2][1])\n",
    "    z14 = keras.Input(shape=(1,), name=coords[2][2])\n",
    "    z15 = keras.Input(shape=(1,), name=coords[2][3])\n",
    "    z16 = keras.Input(shape=(1,), name=coords[2][4])\n",
    "    z17 = keras.Input(shape=(1,), name=coords[2][5]) \n",
    "    z18 = keras.Input(shape=(1,), name=coords[2][6])\n",
    "    z19 = keras.Input(shape=(1,), name=coords[2][7])\n",
    "    z20 = keras.Input(shape=(1,), name=coords[2][8])\n",
    "    z21 = keras.Input(shape=(1,), name=coords[2][9])\n",
    "    z22 = keras.Input(shape=(1,), name=coords[2][10])\n",
    "    z23 = keras.Input(shape=(1,), name=coords[2][11])\n",
    "    z24 = keras.Input(shape=(1,), name=coords[2][12])\n",
    "    z25 = keras.Input(shape=(1,), name=coords[2][13])\n",
    "    z26 = keras.Input(shape=(1,), name=coords[2][14])\n",
    "    z27 = keras.Input(shape=(1,), name=coords[2][15])\n",
    "    z28 = keras.Input(shape=(1,), name=coords[2][16])\n",
    "    z29 = keras.Input(shape=(1,), name=coords[2][17])\n",
    "    z30 = keras.Input(shape=(1,), name=coords[2][18])\n",
    "    z31 = keras.Input(shape=(1,), name=coords[2][19])\n",
    "    z32 = keras.Input(shape=(1,), name=coords[2][20])\n",
    "    z33 = keras.Input(shape=(1,), name=coords[2][21])\n",
    "\n",
    "    v12 = keras.Input(shape=(1,), name=coords[3][0])\n",
    "    v13 = keras.Input(shape=(1,), name=coords[3][1])\n",
    "    v14 = keras.Input(shape=(1,), name=coords[3][2])\n",
    "    v15 = keras.Input(shape=(1,), name=coords[3][3])\n",
    "    v16 = keras.Input(shape=(1,), name=coords[3][4])\n",
    "    v17 = keras.Input(shape=(1,), name=coords[3][5]) \n",
    "    v18 = keras.Input(shape=(1,), name=coords[3][6])\n",
    "    v19 = keras.Input(shape=(1,), name=coords[3][7])\n",
    "    v20 = keras.Input(shape=(1,), name=coords[3][8])\n",
    "    v21 = keras.Input(shape=(1,), name=coords[3][9])\n",
    "    v22 = keras.Input(shape=(1,), name=coords[3][10])\n",
    "    v23 = keras.Input(shape=(1,), name=coords[3][11])\n",
    "    v24 = keras.Input(shape=(1,), name=coords[3][12])\n",
    "    v25 = keras.Input(shape=(1,), name=coords[3][13])\n",
    "    v26 = keras.Input(shape=(1,), name=coords[3][14])\n",
    "    v27 = keras.Input(shape=(1,), name=coords[3][15])\n",
    "    v28 = keras.Input(shape=(1,), name=coords[3][16])\n",
    "    v29 = keras.Input(shape=(1,), name=coords[3][17])\n",
    "    v30 = keras.Input(shape=(1,), name=coords[3][18])\n",
    "    v31 = keras.Input(shape=(1,), name=coords[3][19])\n",
    "    v32 = keras.Input(shape=(1,), name=coords[3][20])\n",
    "    v33 = keras.Input(shape=(1,), name=coords[3][21])\n",
    "\n",
    "    all_inputs = [\n",
    "        x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, \n",
    "        x23, x24, x25, x26, x27, x28, x29, x30, x31, x32, x33,\n",
    "\n",
    "        y12, y13, y14, y15, y16, y17, y18, y19, y20, y21, y22, \n",
    "        y23, y24, y25, y26, y27, y28, y29, y30, y31, y32, y33,\n",
    "\n",
    "        z12, z13, z14, z15, z16, z17, z18, z19, z20, z21, z22, \n",
    "        z23, z24, z25, z26, z27, z28, z29, z30, z31, z32, z33,\n",
    "\n",
    "        v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22, \n",
    "        v23, v24, v25, v26, v27, v28, v29, v30, v31, v32, v33,\n",
    "    ]\n",
    "\n",
    "    return all_inputs\n",
    "\n",
    "\n",
    "def specify_encoded_features(train_ds, all_inputs):\n",
    "\n",
    "    encoded = EncodeFeatures()\n",
    "    coords_p = point()\n",
    "\n",
    "    # point 12 ~ 33\n",
    "    # Numerical features\n",
    "    x12 = encoded.numerical_feature(all_inputs[0], coords_p[0][0], train_ds)\n",
    "    x13 = encoded.numerical_feature(all_inputs[1], coords_p[0][1], train_ds)\n",
    "    x14 = encoded.numerical_feature(all_inputs[2], coords_p[0][2], train_ds)\n",
    "    x15 = encoded.numerical_feature(all_inputs[3], coords_p[0][3], train_ds)\n",
    "    x16 = encoded.numerical_feature(all_inputs[4], coords_p[0][4], train_ds)\n",
    "    x17 = encoded.numerical_feature(all_inputs[5], coords_p[0][5], train_ds)\n",
    "    x18 = encoded.numerical_feature(all_inputs[6], coords_p[0][6], train_ds)\n",
    "    x19 = encoded.numerical_feature(all_inputs[7], coords_p[0][7], train_ds)\n",
    "    x20 = encoded.numerical_feature(all_inputs[8], coords_p[0][8], train_ds)\n",
    "    x21 = encoded.numerical_feature(all_inputs[9], coords_p[0][9], train_ds)\n",
    "    x22 = encoded.numerical_feature(all_inputs[10], coords_p[0][10], train_ds)\n",
    "    x23 = encoded.numerical_feature(all_inputs[11], coords_p[0][11], train_ds)\n",
    "    x24 = encoded.numerical_feature(all_inputs[12], coords_p[0][12], train_ds)\n",
    "    x25 = encoded.numerical_feature(all_inputs[13], coords_p[0][13], train_ds)\n",
    "    x26 = encoded.numerical_feature(all_inputs[14], coords_p[0][14], train_ds)\n",
    "    x27 = encoded.numerical_feature(all_inputs[15], coords_p[0][15], train_ds)\n",
    "    x28 = encoded.numerical_feature(all_inputs[16], coords_p[0][16], train_ds)\n",
    "    x29 = encoded.numerical_feature(all_inputs[17], coords_p[0][17], train_ds)\n",
    "    x30 = encoded.numerical_feature(all_inputs[18], coords_p[0][18], train_ds)\n",
    "    x31 = encoded.numerical_feature(all_inputs[19], coords_p[0][19], train_ds)\n",
    "    x32 = encoded.numerical_feature(all_inputs[20], coords_p[0][20], train_ds)\n",
    "    x33 = encoded.numerical_feature(all_inputs[21], coords_p[0][21], train_ds)\n",
    "\n",
    "    y12 = encoded.numerical_feature(all_inputs[22], coords_p[1][0], train_ds)\n",
    "    y13 = encoded.numerical_feature(all_inputs[23], coords_p[1][1], train_ds)\n",
    "    y14 = encoded.numerical_feature(all_inputs[24], coords_p[1][2], train_ds)\n",
    "    y15 = encoded.numerical_feature(all_inputs[25], coords_p[1][3], train_ds)\n",
    "    y16 = encoded.numerical_feature(all_inputs[26], coords_p[1][4], train_ds)\n",
    "    y17 = encoded.numerical_feature(all_inputs[27], coords_p[1][5], train_ds)\n",
    "    y18 = encoded.numerical_feature(all_inputs[28], coords_p[1][6], train_ds)\n",
    "    y19 = encoded.numerical_feature(all_inputs[29], coords_p[1][7], train_ds)\n",
    "    y20 = encoded.numerical_feature(all_inputs[30], coords_p[1][8], train_ds)\n",
    "    y21 = encoded.numerical_feature(all_inputs[31], coords_p[1][9], train_ds)\n",
    "    y22 = encoded.numerical_feature(all_inputs[32], coords_p[1][10], train_ds)\n",
    "    y23 = encoded.numerical_feature(all_inputs[33], coords_p[1][11], train_ds)\n",
    "    y24 = encoded.numerical_feature(all_inputs[34], coords_p[1][12], train_ds)\n",
    "    y25 = encoded.numerical_feature(all_inputs[35], coords_p[1][13], train_ds)\n",
    "    y26 = encoded.numerical_feature(all_inputs[36], coords_p[1][14], train_ds)\n",
    "    y27 = encoded.numerical_feature(all_inputs[37], coords_p[1][15], train_ds)\n",
    "    y28 = encoded.numerical_feature(all_inputs[38], coords_p[1][16], train_ds)\n",
    "    y29 = encoded.numerical_feature(all_inputs[39], coords_p[1][17], train_ds)\n",
    "    y30 = encoded.numerical_feature(all_inputs[40], coords_p[1][18], train_ds)\n",
    "    y31 = encoded.numerical_feature(all_inputs[41], coords_p[1][19], train_ds)\n",
    "    y32 = encoded.numerical_feature(all_inputs[42], coords_p[1][20], train_ds)\n",
    "    y33 = encoded.numerical_feature(all_inputs[43], coords_p[1][21], train_ds)\n",
    "\n",
    "    z12 = encoded.numerical_feature(all_inputs[44], coords_p[2][0], train_ds)\n",
    "    z13 = encoded.numerical_feature(all_inputs[45], coords_p[2][1], train_ds)\n",
    "    z14 = encoded.numerical_feature(all_inputs[46], coords_p[2][2], train_ds)\n",
    "    z15 = encoded.numerical_feature(all_inputs[47], coords_p[2][3], train_ds)\n",
    "    z16 = encoded.numerical_feature(all_inputs[48], coords_p[2][4], train_ds)\n",
    "    z17 = encoded.numerical_feature(all_inputs[49], coords_p[2][5], train_ds)\n",
    "    z18 = encoded.numerical_feature(all_inputs[50], coords_p[2][6], train_ds)\n",
    "    z19 = encoded.numerical_feature(all_inputs[51], coords_p[2][7], train_ds)\n",
    "    z20 = encoded.numerical_feature(all_inputs[52], coords_p[2][8], train_ds)\n",
    "    z21 = encoded.numerical_feature(all_inputs[53], coords_p[2][9], train_ds)\n",
    "    z22 = encoded.numerical_feature(all_inputs[54], coords_p[2][10], train_ds)\n",
    "    z23 = encoded.numerical_feature(all_inputs[55], coords_p[2][11], train_ds)\n",
    "    z24 = encoded.numerical_feature(all_inputs[56], coords_p[2][12], train_ds)\n",
    "    z25 = encoded.numerical_feature(all_inputs[57], coords_p[2][13], train_ds)\n",
    "    z26 = encoded.numerical_feature(all_inputs[58], coords_p[2][14], train_ds)\n",
    "    z27 = encoded.numerical_feature(all_inputs[59], coords_p[2][15], train_ds)\n",
    "    z28 = encoded.numerical_feature(all_inputs[60], coords_p[2][16], train_ds)\n",
    "    z29 = encoded.numerical_feature(all_inputs[61], coords_p[2][17], train_ds)\n",
    "    z30 = encoded.numerical_feature(all_inputs[62], coords_p[2][18], train_ds)\n",
    "    z31 = encoded.numerical_feature(all_inputs[63], coords_p[2][19], train_ds)\n",
    "    z32 = encoded.numerical_feature(all_inputs[64], coords_p[2][20], train_ds)\n",
    "    z33 = encoded.numerical_feature(all_inputs[65], coords_p[2][21], train_ds)\n",
    "\n",
    "    v12 = encoded.numerical_feature(all_inputs[66], coords_p[3][0], train_ds)\n",
    "    v13 = encoded.numerical_feature(all_inputs[67], coords_p[3][1], train_ds)\n",
    "    v14 = encoded.numerical_feature(all_inputs[68], coords_p[3][2], train_ds)\n",
    "    v15 = encoded.numerical_feature(all_inputs[69], coords_p[3][3], train_ds)\n",
    "    v16 = encoded.numerical_feature(all_inputs[70], coords_p[3][4], train_ds)\n",
    "    v17 = encoded.numerical_feature(all_inputs[71], coords_p[3][5], train_ds)\n",
    "    v18 = encoded.numerical_feature(all_inputs[72], coords_p[3][6], train_ds)\n",
    "    v19 = encoded.numerical_feature(all_inputs[73], coords_p[3][7], train_ds)\n",
    "    v20 = encoded.numerical_feature(all_inputs[74], coords_p[3][8], train_ds)\n",
    "    v21 = encoded.numerical_feature(all_inputs[75], coords_p[3][9], train_ds)\n",
    "    v22 = encoded.numerical_feature(all_inputs[76], coords_p[3][10], train_ds)\n",
    "    v23 = encoded.numerical_feature(all_inputs[77], coords_p[3][11], train_ds)\n",
    "    v24 = encoded.numerical_feature(all_inputs[78], coords_p[3][12], train_ds)\n",
    "    v25 = encoded.numerical_feature(all_inputs[79], coords_p[3][13], train_ds)\n",
    "    v26 = encoded.numerical_feature(all_inputs[80], coords_p[3][14], train_ds)\n",
    "    v27 = encoded.numerical_feature(all_inputs[81], coords_p[3][15], train_ds)\n",
    "    v28 = encoded.numerical_feature(all_inputs[82], coords_p[3][16], train_ds)\n",
    "    v29 = encoded.numerical_feature(all_inputs[83], coords_p[3][17], train_ds)\n",
    "    v30 = encoded.numerical_feature(all_inputs[84], coords_p[3][18], train_ds)\n",
    "    v31 = encoded.numerical_feature(all_inputs[85], coords_p[3][19], train_ds)\n",
    "    v32 = encoded.numerical_feature(all_inputs[86], coords_p[3][20], train_ds)\n",
    "    v33 = encoded.numerical_feature(all_inputs[87], coords_p[3][21], train_ds)\n",
    "\n",
    "    all_features = layers.concatenate(\n",
    "        [\n",
    "            x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, \n",
    "            x23, x24, x25, x26, x27, x28, x29, x30, x31, x32, x33,\n",
    "\n",
    "            y12, y13, y14, y15, y16, y17, y18, y19, y20, y21, y22,\n",
    "            y23, y24, y25, y26, y27, y28, y29, y30, y31, y32, y33,\n",
    "\n",
    "            z12, z13, z14, z15, z16, z17, z18, z19, z20, z21, z22, \n",
    "            z23, z24, z25, z26, z27, z28, z29, z30, z31, z32, z33,\n",
    "\n",
    "            v12, v13, v14, v15, v16, v17, v18, v19, v20, v21, v22,\n",
    "            v23, v24, v25, v26, v27, v28, v29, v30, v31, v32, v33,\n",
    "        ]\n",
    "    )\n",
    "    return all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 - 6s - loss: 0.9913 - accuracy: 0.6349 - val_loss: 0.4120 - val_accuracy: 0.9164 - 6s/epoch - 46ms/step\n",
      "Epoch 2/100\n",
      "132/132 - 2s - loss: 0.4251 - accuracy: 0.8702 - val_loss: 0.2248 - val_accuracy: 0.9677 - 2s/epoch - 12ms/step\n",
      "Epoch 3/100\n",
      "132/132 - 2s - loss: 0.3034 - accuracy: 0.9200 - val_loss: 0.1525 - val_accuracy: 0.9905 - 2s/epoch - 12ms/step\n",
      "Epoch 4/100\n",
      "132/132 - 2s - loss: 0.2330 - accuracy: 0.9445 - val_loss: 0.1097 - val_accuracy: 0.9934 - 2s/epoch - 12ms/step\n",
      "Epoch 5/100\n",
      "132/132 - 2s - loss: 0.1971 - accuracy: 0.9499 - val_loss: 0.0849 - val_accuracy: 0.9943 - 2s/epoch - 12ms/step\n",
      "Epoch 6/100\n",
      "132/132 - 2s - loss: 0.1642 - accuracy: 0.9649 - val_loss: 0.0674 - val_accuracy: 0.9953 - 2s/epoch - 12ms/step\n",
      "Epoch 7/100\n",
      "132/132 - 2s - loss: 0.1446 - accuracy: 0.9661 - val_loss: 0.0540 - val_accuracy: 0.9962 - 2s/epoch - 12ms/step\n",
      "Epoch 8/100\n",
      "132/132 - 2s - loss: 0.1306 - accuracy: 0.9694 - val_loss: 0.0446 - val_accuracy: 0.9962 - 2s/epoch - 12ms/step\n",
      "Epoch 9/100\n",
      "132/132 - 2s - loss: 0.1174 - accuracy: 0.9708 - val_loss: 0.0388 - val_accuracy: 0.9962 - 2s/epoch - 12ms/step\n",
      "Epoch 10/100\n",
      "132/132 - 2s - loss: 0.1118 - accuracy: 0.9739 - val_loss: 0.0328 - val_accuracy: 0.9972 - 2s/epoch - 12ms/step\n",
      "Epoch 11/100\n",
      "132/132 - 2s - loss: 0.0993 - accuracy: 0.9760 - val_loss: 0.0287 - val_accuracy: 0.9972 - 2s/epoch - 12ms/step\n",
      "Epoch 12/100\n",
      "132/132 - 2s - loss: 0.0968 - accuracy: 0.9739 - val_loss: 0.0256 - val_accuracy: 0.9981 - 2s/epoch - 12ms/step\n",
      "Epoch 13/100\n",
      "132/132 - 2s - loss: 0.0861 - accuracy: 0.9812 - val_loss: 0.0231 - val_accuracy: 0.9981 - 2s/epoch - 12ms/step\n",
      "Epoch 14/100\n",
      "132/132 - 2s - loss: 0.0816 - accuracy: 0.9817 - val_loss: 0.0208 - val_accuracy: 0.9991 - 2s/epoch - 12ms/step\n",
      "Epoch 15/100\n",
      "132/132 - 2s - loss: 0.0784 - accuracy: 0.9822 - val_loss: 0.0187 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 16/100\n",
      "132/132 - 2s - loss: 0.0725 - accuracy: 0.9829 - val_loss: 0.0171 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 17/100\n",
      "132/132 - 2s - loss: 0.0688 - accuracy: 0.9843 - val_loss: 0.0158 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 18/100\n",
      "132/132 - 2s - loss: 0.0676 - accuracy: 0.9836 - val_loss: 0.0146 - val_accuracy: 0.9991 - 2s/epoch - 12ms/step\n",
      "Epoch 19/100\n",
      "132/132 - 2s - loss: 0.0678 - accuracy: 0.9810 - val_loss: 0.0136 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 20/100\n",
      "132/132 - 2s - loss: 0.0604 - accuracy: 0.9831 - val_loss: 0.0125 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 21/100\n",
      "132/132 - 2s - loss: 0.0583 - accuracy: 0.9855 - val_loss: 0.0119 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 22/100\n",
      "132/132 - 2s - loss: 0.0581 - accuracy: 0.9865 - val_loss: 0.0110 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 23/100\n",
      "132/132 - 2s - loss: 0.0563 - accuracy: 0.9843 - val_loss: 0.0107 - val_accuracy: 1.0000 - 2s/epoch - 13ms/step\n",
      "Epoch 24/100\n",
      "132/132 - 2s - loss: 0.0522 - accuracy: 0.9874 - val_loss: 0.0098 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 25/100\n",
      "132/132 - 2s - loss: 0.0536 - accuracy: 0.9874 - val_loss: 0.0094 - val_accuracy: 1.0000 - 2s/epoch - 13ms/step\n",
      "Epoch 26/100\n",
      "132/132 - 2s - loss: 0.0541 - accuracy: 0.9853 - val_loss: 0.0095 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 27/100\n",
      "132/132 - 2s - loss: 0.0477 - accuracy: 0.9877 - val_loss: 0.0089 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 28/100\n",
      "132/132 - 2s - loss: 0.0519 - accuracy: 0.9877 - val_loss: 0.0082 - val_accuracy: 1.0000 - 2s/epoch - 13ms/step\n",
      "Epoch 29/100\n",
      "132/132 - 2s - loss: 0.0479 - accuracy: 0.9877 - val_loss: 0.0078 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 30/100\n",
      "132/132 - 2s - loss: 0.0424 - accuracy: 0.9903 - val_loss: 0.0073 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 31/100\n",
      "132/132 - 2s - loss: 0.0471 - accuracy: 0.9877 - val_loss: 0.0072 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 32/100\n",
      "132/132 - 2s - loss: 0.0403 - accuracy: 0.9888 - val_loss: 0.0067 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 33/100\n",
      "132/132 - 2s - loss: 0.0420 - accuracy: 0.9884 - val_loss: 0.0068 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 34/100\n",
      "132/132 - 2s - loss: 0.0421 - accuracy: 0.9893 - val_loss: 0.0063 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 35/100\n",
      "132/132 - 2s - loss: 0.0406 - accuracy: 0.9903 - val_loss: 0.0062 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 36/100\n",
      "132/132 - 2s - loss: 0.0407 - accuracy: 0.9896 - val_loss: 0.0058 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 37/100\n",
      "132/132 - 2s - loss: 0.0387 - accuracy: 0.9884 - val_loss: 0.0057 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 38/100\n",
      "132/132 - 2s - loss: 0.0375 - accuracy: 0.9907 - val_loss: 0.0056 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 39/100\n",
      "132/132 - 2s - loss: 0.0354 - accuracy: 0.9915 - val_loss: 0.0055 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 40/100\n",
      "132/132 - 2s - loss: 0.0387 - accuracy: 0.9893 - val_loss: 0.0052 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 41/100\n",
      "132/132 - 2s - loss: 0.0363 - accuracy: 0.9912 - val_loss: 0.0050 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 42/100\n",
      "132/132 - 2s - loss: 0.0379 - accuracy: 0.9881 - val_loss: 0.0049 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 43/100\n",
      "132/132 - 2s - loss: 0.0353 - accuracy: 0.9910 - val_loss: 0.0048 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 44/100\n",
      "132/132 - 2s - loss: 0.0336 - accuracy: 0.9905 - val_loss: 0.0045 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 45/100\n",
      "132/132 - 2s - loss: 0.0380 - accuracy: 0.9888 - val_loss: 0.0043 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 46/100\n",
      "132/132 - 2s - loss: 0.0317 - accuracy: 0.9915 - val_loss: 0.0042 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 47/100\n",
      "132/132 - 2s - loss: 0.0289 - accuracy: 0.9941 - val_loss: 0.0042 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 48/100\n",
      "132/132 - 2s - loss: 0.0312 - accuracy: 0.9931 - val_loss: 0.0041 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 49/100\n",
      "132/132 - 2s - loss: 0.0278 - accuracy: 0.9931 - val_loss: 0.0040 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 50/100\n",
      "132/132 - 2s - loss: 0.0308 - accuracy: 0.9931 - val_loss: 0.0038 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 51/100\n",
      "132/132 - 2s - loss: 0.0332 - accuracy: 0.9900 - val_loss: 0.0038 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 52/100\n",
      "132/132 - 2s - loss: 0.0278 - accuracy: 0.9929 - val_loss: 0.0036 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 53/100\n",
      "132/132 - 2s - loss: 0.0251 - accuracy: 0.9948 - val_loss: 0.0036 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 54/100\n",
      "132/132 - 2s - loss: 0.0255 - accuracy: 0.9938 - val_loss: 0.0036 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 55/100\n",
      "132/132 - 2s - loss: 0.0345 - accuracy: 0.9907 - val_loss: 0.0034 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 56/100\n",
      "132/132 - 2s - loss: 0.0272 - accuracy: 0.9934 - val_loss: 0.0034 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 57/100\n",
      "132/132 - 2s - loss: 0.0293 - accuracy: 0.9931 - val_loss: 0.0033 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 58/100\n",
      "132/132 - 2s - loss: 0.0293 - accuracy: 0.9929 - val_loss: 0.0032 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 59/100\n",
      "132/132 - 2s - loss: 0.0271 - accuracy: 0.9943 - val_loss: 0.0031 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 60/100\n",
      "132/132 - 2s - loss: 0.0234 - accuracy: 0.9934 - val_loss: 0.0031 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 61/100\n",
      "132/132 - 2s - loss: 0.0256 - accuracy: 0.9941 - val_loss: 0.0030 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 62/100\n",
      "132/132 - 2s - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.0029 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 63/100\n",
      "132/132 - 2s - loss: 0.0224 - accuracy: 0.9950 - val_loss: 0.0028 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 64/100\n",
      "132/132 - 2s - loss: 0.0254 - accuracy: 0.9953 - val_loss: 0.0028 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 65/100\n",
      "132/132 - 2s - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.0028 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 66/100\n",
      "132/132 - 2s - loss: 0.0246 - accuracy: 0.9955 - val_loss: 0.0027 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 67/100\n",
      "132/132 - 2s - loss: 0.0194 - accuracy: 0.9960 - val_loss: 0.0027 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 68/100\n",
      "132/132 - 2s - loss: 0.0244 - accuracy: 0.9943 - val_loss: 0.0026 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 69/100\n",
      "132/132 - 2s - loss: 0.0258 - accuracy: 0.9936 - val_loss: 0.0026 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 70/100\n",
      "132/132 - 2s - loss: 0.0259 - accuracy: 0.9941 - val_loss: 0.0026 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 71/100\n",
      "132/132 - 2s - loss: 0.0234 - accuracy: 0.9931 - val_loss: 0.0026 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 72/100\n",
      "132/132 - 2s - loss: 0.0240 - accuracy: 0.9934 - val_loss: 0.0025 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 73/100\n",
      "132/132 - 2s - loss: 0.0221 - accuracy: 0.9943 - val_loss: 0.0025 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 74/100\n",
      "132/132 - 2s - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.0024 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 75/100\n",
      "132/132 - 2s - loss: 0.0234 - accuracy: 0.9941 - val_loss: 0.0023 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 76/100\n",
      "132/132 - 2s - loss: 0.0218 - accuracy: 0.9945 - val_loss: 0.0023 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 77/100\n",
      "132/132 - 2s - loss: 0.0238 - accuracy: 0.9938 - val_loss: 0.0023 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 78/100\n",
      "132/132 - 2s - loss: 0.0220 - accuracy: 0.9938 - val_loss: 0.0022 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 79/100\n",
      "132/132 - 2s - loss: 0.0224 - accuracy: 0.9957 - val_loss: 0.0022 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 80/100\n",
      "132/132 - 2s - loss: 0.0257 - accuracy: 0.9917 - val_loss: 0.0022 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 81/100\n",
      "132/132 - 2s - loss: 0.0228 - accuracy: 0.9945 - val_loss: 0.0022 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 82/100\n",
      "132/132 - 2s - loss: 0.0214 - accuracy: 0.9941 - val_loss: 0.0021 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 83/100\n",
      "132/132 - 2s - loss: 0.0173 - accuracy: 0.9974 - val_loss: 0.0020 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 84/100\n",
      "132/132 - 2s - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.0020 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 85/100\n",
      "132/132 - 2s - loss: 0.0216 - accuracy: 0.9934 - val_loss: 0.0020 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 86/100\n",
      "132/132 - 2s - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.0020 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 87/100\n",
      "132/132 - 2s - loss: 0.0190 - accuracy: 0.9960 - val_loss: 0.0019 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 88/100\n",
      "132/132 - 2s - loss: 0.0193 - accuracy: 0.9964 - val_loss: 0.0019 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 89/100\n",
      "132/132 - 2s - loss: 0.0216 - accuracy: 0.9941 - val_loss: 0.0018 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 90/100\n",
      "132/132 - 2s - loss: 0.0204 - accuracy: 0.9962 - val_loss: 0.0018 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 91/100\n",
      "132/132 - 2s - loss: 0.0209 - accuracy: 0.9943 - val_loss: 0.0017 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 92/100\n",
      "132/132 - 2s - loss: 0.0189 - accuracy: 0.9957 - val_loss: 0.0017 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 93/100\n",
      "132/132 - 2s - loss: 0.0192 - accuracy: 0.9960 - val_loss: 0.0018 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 94/100\n",
      "132/132 - 2s - loss: 0.0198 - accuracy: 0.9943 - val_loss: 0.0017 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 95/100\n",
      "132/132 - 2s - loss: 0.0184 - accuracy: 0.9972 - val_loss: 0.0017 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 96/100\n",
      "132/132 - 2s - loss: 0.0190 - accuracy: 0.9948 - val_loss: 0.0016 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 97/100\n",
      "132/132 - 2s - loss: 0.0169 - accuracy: 0.9967 - val_loss: 0.0016 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 98/100\n",
      "132/132 - 2s - loss: 0.0165 - accuracy: 0.9969 - val_loss: 0.0015 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 99/100\n",
      "132/132 - 2s - loss: 0.0174 - accuracy: 0.9967 - val_loss: 0.0015 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "Epoch 100/100\n",
      "132/132 - 2s - loss: 0.0205 - accuracy: 0.9957 - val_loss: 0.0015 - val_accuracy: 1.0000 - 2s/epoch - 12ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\RAIKIB~1\\AppData\\Local\\Temp\\tmpj7fswxhq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Model save done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_csv_file = 'data.csv'\n",
    "target_value = 'class'\n",
    "all_model = './model_weights/all_model/08.31_xyzv/3_categories_pose' # all_model: Model struct and model weights.\n",
    "\n",
    "# Data preprocessed and creat datasets.\n",
    "pose_datasets = CsvDataset(file=dataset_csv_file)\n",
    "df_pose = pose_datasets.csv_preprocessing()\n",
    "train_ds, val_ds = pose_datasets.df_to_datasets(dataframe=df_pose, target=target_value)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)\n",
    "\n",
    "    # Functional API model build.\n",
    "all_inputs = input_features()\n",
    "all_features = specify_encoded_features(train_ds, all_inputs)\n",
    "x = layers.Dense(32, activation='relu')(all_features)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "    # output = layers.Dense(3, activation='sigmoid')(x) # from_logits=False\n",
    "output = layers.Dense(6, activation='softmax')(x) # from_logits=False\n",
    "model = keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(optimizer='sgd', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "    # Model train.\n",
    "model.fit(x=train_ds, epochs=100, verbose=2, validation_data=val_ds)\n",
    "\n",
    "    # # Option1: Save Keras model. # #\n",
    "    # model.save(all_model)\n",
    "    # print('Model save done!')\n",
    "\n",
    "    # # Option2: Save TFLite model. # #\n",
    "    # Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# Save the model.\n",
    "with open('softmaxmodel.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print('TFLite Model save done!')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
